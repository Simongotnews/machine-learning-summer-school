{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with pretrained CNNs\n",
    "\n",
    "In this notebook we want to use a CNN with your own image dataset. So far we used the Fashion MNIST dataset to create a model. \n",
    "\n",
    "This time it's your turn to obtain data!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Obtaining Data\n",
    "\n",
    "We already provide two image collections on the server for you.\n",
    "\n",
    "- In the folder [*/data/image_cats_dogs*](/tree/data/image_cats_dogs) are 100 cat and 100 dog images. They come from the Kaggle-Challenge [*Dogs vs. Cats*](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "- In the folder [*/data/image_lion_puma*](/tree/data/image_lion_puma) are some images of lions and pumas, intentionally badly chosen to show the limits of learning methods.  \n",
    "- If you trust yourself to create your own dataset, you can do it by the same procedure. For example you could develope a George Clooney vs. Brad Pitt classifier. Therefore you only have to save some images of them on the server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our current path\n",
    "print(\"current path: {}\".format(os.getcwd()))\n",
    "\n",
    "# let's save the home path\n",
    "home = \"/home/jovyan/\"\n",
    "\n",
    "folders = {home + \"data/image_cats_dogs/\",home + \"data/image_lion_puma/train/puma/\"}\n",
    "for folder in folders:\n",
    "    for i, file in zip(range(3),os.listdir(folder)):\n",
    "        display(Image(filename=(folder + file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "For further steps it's easier to separate the image classes into different directories. The most toolkits are able to map the name of a directory to a specfic class. For the puma-lion example the images are already separated. For the cat-dog example we have to do this with some lines of code.\n",
    "\n",
    "If you haven't already done it, you should execute the exercise 1.2 for image processing the cat vs. dog dataset.\n",
    "After that we can start with this notebook and compare a simple CNN with a pretrained CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model \n",
    "\n",
    "Finally we can start with the real neural network itself. Therefore we need the deep learning libraries and create some constants for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "img_size = 250\n",
    "\n",
    "num_train = 160\n",
    "num_test = 40\n",
    "\n",
    "train_data_dir = '/home/jovyan/temp/image_cats_dogs/train'\n",
    "test_data_dir  = '/home/jovyan/temp/image_cats_dogs/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We define a sequential network, which passes the data from input to the direction of output. \n",
    "\n",
    "The components of the Convolutional Network will be the same, as you've already seen in the exercises before.\n",
    "\n",
    "***Hints:***\n",
    "- the required elements are `Conv2D`, `Activation`, `MaxPooling`, `Flatten` and `Dense`\n",
    "- we have to compile the network definition with `Sequential.compile` and define the loss function as well as an optimizer \n",
    "- `Sequential.summary()` summarizes the layer and the number of degrees of freedom( = learning parameter)\n",
    "- start with two Convolutional Layer and reduce the dimension with Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the backend the order of dimension parameter is different\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_size, img_size)\n",
    "else:\n",
    "    input_shape = (img_size, img_size, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (7, 7), input_shape=input_shape))\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training process\n",
    "\n",
    "Now we have to combine the data with the network. This is the same process like in the Fashion MNIST example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12 # so that our data augmentation makes sense we want to see each image 12 times\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be directly fed with data (`fit`) or can get the data from a generator (`fit_generator`). As we want to increase our data on-the-fly with augmentation, we define this generator and derive another generator form this previous generator, which reads the data in the training folder.  \n",
    "\n",
    "***Hints:***\n",
    "- The interger color values (0-255) should be normalized in the range from 0 to 1 with the `rescale` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation and generator\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can start the training process. Therefore we have to implement the `fit_generator` method with the training data generator and the desired number of epochs. After that we wait until the training is finished...;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs)\n",
    "\n",
    "model.save_weights('MyCatDogConv.h5') # Save the state in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Progress of accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "Now we want to test the performance of our model for the test data. There is a method analogous to `fit_generator`, called `evaluate_generator`. \n",
    "\n",
    "***Hint*** Don't forget`rescaling` and set  `shuffle` to `false` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print(\"loss: {}, Acc: {} \".format(scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the confusion matrix we need the individual outputs (predictions) of the network. Therefore we can use `model.predict_generator`. Here we don't get a binary value, but a value between 0 and 1. So we have to check against a threshold (usually 0.5, but depending on the application one class can be more important.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# we know that the test data starts with all cats and then all dogs\n",
    "truth = np.concatenate([np.zeros(20),np.ones(20)])\n",
    "\n",
    "# threshold = 0.5 --> round\n",
    "preds = np.around(predictions)\n",
    "\n",
    "# the common thing:\n",
    "print(\"Accuracy: {}\".format(skm.accuracy_score(truth,preds)))\n",
    "cm = skm.confusion_matrix(truth,preds)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(labels.values()))\n",
    "ax.set_yticklabels(['']+list(labels.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "With the limited datasets (cats vs. dogs and lions vs. pumas) the values probably wouldn't get better. Therefore this task is too difficult because the examples have to be learned from zero. An alternative is taking a pretrained network, which is already trained on some image features and it can recognize them. We only take the last part of this pretrained CNN (Dense layers after convolutional layer, also called *Bottleneck*) and retrain them with our data according to our needs. \n",
    "\n",
    "Therefore we download the current and already trained CNN (in our case: VGG16) without the upper layer and calculate the CNN output for our data without having trained it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "import numpy as np\n",
    "\n",
    "# no augmentation\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Pretrained CNN without the upper layers \n",
    "vgg16 = applications.VGG16(include_top=False, weights='imagenet', input_shape=(250,250,3))\n",
    "\n",
    "# Process the training data and save the ouput in a file \n",
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_train = vgg16.predict_generator(\n",
    "        generator, num_train // batch_size)\n",
    "np.save(open(\"bottleneck_features_train.npy\", \"wb\"), bottleneck_features_train)\n",
    "\n",
    "# The same for test data\n",
    "generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_test = vgg16.predict_generator(\n",
    "        generator, num_test // batch_size)\n",
    "np.save(open(\"bottleneck_features_test.npy\", \"wb\"),\n",
    "            bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we learn in that basis a simple neural net and call it 'topModel'. This time we directly integrate the evaluation into the training process by defining the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CNN output again\n",
    "train_data = np.load(open(\"bottleneck_features_train.npy\", \"rb\"))\n",
    "train_labels = np.array([0] * (num_train // 2) + [1] * (num_train // 2))\n",
    "test_data = np.load(open(\"bottleneck_features_test.npy\", \"rb\"))\n",
    "test_labels = np.array([0] * (num_test // 2) + [1] * (num_test // 2))\n",
    "\n",
    "# define a small simple CNN\n",
    "topModel = Sequential()\n",
    "...\n",
    "#Flatten, Dense, Dropout, Dense with Sigmoid\n",
    "\n",
    "topModel.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# and train it\n",
    "topModel.fit(train_data, train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_data, test_labels))\n",
    "\n",
    "topModel.save_weights(\"top_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have with less augmented data great results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
