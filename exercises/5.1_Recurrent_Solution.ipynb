{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "We previously examined how we can be more flexible with the input sizes and exploit spatial proximity and invariance the features with convolutional neural networks.\n",
    "\n",
    "Recurrent Neural Networks can do this, too, but have a different perspective on the data: More loosely coupled sequences of inputs and outputs and an internal state (see slides).\n",
    "\n",
    "With this we once more tackle the Fashion MNIST data as an easy example, and then try to teach the computer to generate character sequences.\n",
    "\n",
    "## The usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the same data as before, so we included this step already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print (\"y_train.shape = {}\".format(y_train.shape))\n",
    "print (\"x_train.shape = {}\".format(x_train.shape))\n",
    "\n",
    "print(len(y_train), 'train samples')\n",
    "print(len(y_test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 10 categories in our data\n",
    "num_class = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Normalizing color values\n",
    "x_train = x_train / float(255)\n",
    "x_test  = x_test / float(255)\n",
    "\n",
    "# one hot vectors of the shape 10 ()\n",
    "y_train_hot = np_utils.to_categorical(y_train, num_class)\n",
    "y_test_hot  = np_utils.to_categorical(y_test, num_class)\n",
    "\n",
    "# Shapes \n",
    "print (\"y_train_hot.shape = {}\".format(y_train_hot.shape))\n",
    "print (\"x_train.shape = {}\".format(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "This time we want to process the images one row, while always feeding our internal state as an input into the next row. This way, when we reach the last row, we have built a state that combines all the information of the image.\n",
    "\n",
    "We want to use an LSTM Layer with multiple nodes. The sequence or time steps would be the rows of the image, while each row has 28 pixels as features.\n",
    "\n",
    "***Hints:***\n",
    "- the `LSTM` layer needs an 3D or 2D input shape (#batchSize, #timeSteps, #features), but batchsize is optional. Time steps means the number of inputs in one training example.\n",
    "- End the model stack with a dense softmax layer as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Parameters for LSTM network\n",
    "lstmLayerWidth = 30\n",
    "timeSteps = img_rows\n",
    "dim_input_vector = img_cols\n",
    "\n",
    "input_shape = (timeSteps, dim_input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstmLayerWidth, input_shape=input_shape))\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Same as in earlier exercises ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(x_train, y_train_hot, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "The same procedure as last time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation = model.evaluate(x_test, y_test_hot, batch_size=batch_size, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))\n",
    "\n",
    "predictions = model.predict_classes(x_test);\n",
    "cm = metrics.confusion_matrix(y_test ,predictions)\n",
    "\n",
    "# Output\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the progression shows, we could probably improve this model with more training epochs.\n",
    "However, we want to try now other kind of problems that are not solvable with non-recurrent networks. Let's see what's this about:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorizing a sequence\n",
    "\n",
    "This is a dummy use case, but it shows nicely how hard it is to memorize a sentence, if you are only being told how wrong you are. The goal is to create a model than can recreate a complete sentence from only the first character. This sentence is \n",
    "\n",
    "***Der Cottbuser Postkutscher putzt den Cottbuser Postkutschkasten***\n",
    "\n",
    "The model will only get one character as input and has to create the next character. What makes this special is, that the input ***t*** has different outputs (*t,b,k,s,z*) and the correct one depends on what has been observed up to that point in the sequence. This information will reside in the internal states of the LSTM cells.\n",
    "\n",
    "Let's import some necessary things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, GRU, Input\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training data\n",
    "\n",
    "Now we define the learning goal, i.e. the string to repeat. The first character is special as it will function as a initiator to start the repetition once the training is complete.\n",
    "\n",
    "Similar to processing words, we have to create a *vocabulary* of possible characters from that string in order to represent them in the numerical input vectors that we feed into the network.\n",
    "\n",
    "We also predefined the number of nodes per hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_STRING = \"*Der Cottbuser Postkutscher putzt den Cottbuser Postkutschkasten.\"\n",
    "VOCAB = list(set(LEARN_STRING))\n",
    "\n",
    "NUM_FEATURES = len(VOCAB) # dimensions of the input = one character as one hot vector\n",
    "HIDDEN_WIDTH = 50         # how many nodes per hidden layer = arbitrary number, could be bigger or smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the actual input data is just the sequence of characters and the corresponding, desired output is the next character in line, i.e. the y vector is just the same as the x vector but shifted by one position.\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "|  row | x   | y   |\n",
    "|------|-----|-----|\n",
    "| 1    | *   | C   |\n",
    "| 2    | C   | o   |\n",
    "| 3    | o   | t   |\n",
    "| 4    | t   | t   |\n",
    "| 5    | t   | b   |\n",
    "| 6    | b   | u   |\n",
    "| 7    | u   | s   |\n",
    "| ...  | ... | ... |\n",
    "| n    | .   | *   |\n",
    "\n",
    "But with one hot vectors instead of the characters. As we have seen in the MNIT example above, the RNN needs the data in the shape of (batch_size, timesteps, features). We have only one training sequence, therefore the batchsize is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(LEARN_STRING))\n",
    "print(len(VOCAB))\n",
    "\n",
    "# create two numpy arrays with the correct amount of values (zeroes for initialization)\n",
    "# and reshape them into the desired input shape\n",
    "x = np.zeros(len(VOCAB)*len(LEARN_STRING)).reshape(1, len(LEARN_STRING), len(VOCAB))\n",
    "y = np.zeros(len(VOCAB)*len(LEARN_STRING)).reshape(1, len(LEARN_STRING), len(VOCAB))\n",
    "\n",
    "# fill them with training data: \n",
    "# one input character as one hot vector -> one output character as one hot vector\n",
    "for i, char in zip(range(len(LEARN_STRING)), LEARN_STRING):\n",
    "    x[0][i][VOCAB.index(char)] = 1\n",
    "    y[0][(i-1)%len(LEARN_STRING)][VOCAB.index(char)] = 1\n",
    "\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short check if the sequences match in the way we want to:\n",
    "x_string = \"\"\n",
    "y_string = \"\";\n",
    "for i in range(len(LEARN_STRING)):\n",
    "    x_string += (VOCAB[np.argmax( x[0,i,:])])\n",
    "    y_string += (VOCAB[np.argmax( y[0,i,:])])\n",
    "print(x_string)\n",
    "print(y_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "Sequence-to-sequence learning and predicting is a bit trickier: \n",
    "- We want to learn from the whole sequence, not individual character pairs. The model shall learn the influence of the internal state, too.\n",
    "- During prediction we only feed it one character at the time and build up the sequence from the output of the model itself.\n",
    "\n",
    "This means we have to define the same network in slightly different but compatible configurations. Therefore, we define a procedure to build the model and call it later with different parameters.\n",
    "\n",
    "We provided the interface and the building the model. You have to define the layers.\n",
    "\n",
    "***Hints:***\n",
    "- Take a look at the layers we imported at the beginning of this section.\n",
    "- use `batch_input_shape` for the first layer, to define the input shape. This will be needed later on.\n",
    "- The `LSTM` layers need to `return_sequences`, otherwise they produce only one output at the end of the sequence. \n",
    "- The \"statefulness\" of the layers needs to be set according to the provided parameter\n",
    "- The output layer needs to be encapsulated into a `TimeDistributed`, which means that it collects one output for each input throughout the whole sequence.\n",
    "- Instead of LSTM, you can also use `GRU` layers which have the exactly same interface but learn a bit better in this use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeDistributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_timesteps, num_features, num_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(num_hidden_nodes, \n",
    "                     batch_input_shape=(batch_size, num_timesteps, num_features),\n",
    "                     return_sequences=True, #produce output for each word, not just last one\n",
    "                     stateful=stateful))\n",
    "    model.add(GRU(num_hidden_nodes,\n",
    "                     return_sequences=True, #produce output for each word, not just last one\n",
    "                     stateful=stateful))\n",
    "    model.add(TimeDistributed(Dense(num_features, activation=\"softmax\"), \n",
    "                                   name='output_layer'))\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the training model\n",
    "\n",
    "We now create a training model with our method `create_model` using the constant values defined above. The length of the training string is the number of time steps in one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training model\n",
    "train_model = create_model(len(LEARN_STRING), NUM_FEATURES, HIDDEN_WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only have one sample, we have to go through many epochs. You can set `verbose` to `False` in order to not get spammed with stats. Let's start with 100 epochs and see what we get. (Spoiler: 100 will not be enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to data for some epochs\n",
    "train_model.fit(x, y, epochs=100, batch_size=1, verbose=False)\n",
    "train_model.fit(x, y, epochs=1, batch_size=1) # one last quick epoch to get the current stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already said: the training model will not help us when we want to make predictions because it only accepts a whole sequence as input and it resets it's state after each prediction.\n",
    "\n",
    "Therefore we create a structurally similar model, but we make it `stateful` which means we control when the states are being reset, and we set the batch size and sequence length to one.\n",
    "\n",
    "Of course this new model is untrained, but we can `set_weights` to the values of the already trained one :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediciont model and take over weights\n",
    "predict_model = create_model(1, NUM_FEATURES, HIDDEN_WIDTH, batch_size=1, stateful=True)\n",
    "predict_model.set_weights(train_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making predictions we create an initial single input from the first character of the target string (Hint: shape(1,1,NUM_FEATURES)) and reset the state of the model.\n",
    "\n",
    "Now you can use the output of the one prediction as input into the next prediction and collect all the characters which get created this way. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token\n",
    "start = np.zeros(len(VOCAB)).reshape(1,1,len(VOCAB))\n",
    "start[0][0][VOCAB.index(LEARN_STRING[0])] = 1\n",
    "\n",
    "# init\n",
    "predict_model.reset_states()\n",
    "input_char = start\n",
    "\n",
    "result = \"\"\n",
    "for i in range(len(LEARN_STRING)):\n",
    "    input_char = predict_model.predict(input_char)\n",
    "    result += VOCAB[np.argmax(input_char)]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first try will be a disaster. But if it produces more than just a single repeated all over, you are on the right track! Just make more epochs, recreate the prediction model and retry. \n",
    "\n",
    "50 epochs at a time should show a nice progression. 500 epochs should solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_per_step = 30\n",
    "for run in range(500//epochs_per_step):\n",
    "    # make some epochs more\n",
    "    history = train_model.fit(x, y, epochs=epochs_per_step, batch_size=1, verbose=False)\n",
    "    # transfer weights\n",
    "    predict_model.set_weights(train_model.get_weights())\n",
    "    # create output\n",
    "    predict_model.reset_states()\n",
    "    input_char = start\n",
    "    result = \"\"\n",
    "    for j in range(len(LEARN_STRING)):\n",
    "        input_char = predict_model.predict(input_char)\n",
    "        result += VOCAB[np.argmax(input_char)]\n",
    "    print(\"Result after\", ((run+1)*epochs_per_step+100), \"epochs:\",result)\n",
    "    if ((\"*\"+result) == (LEARN_STRING+\"*\")): break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
