{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using Neural Networks\n",
    "\n",
    "After having the chance to try different parameters in the Tensorflow Playground, now it's our turn to implement something by ourselves using neural networks. As you may remember we already worked with the Fashion-MNIST dataset using unsupervised methods. Today we're going to use Keras in order to build our model, this time supervised classification. Keras is a high level framework for machine learning, which uses Tensorflow as backend. It allows us to implement neural network in a very confortable form. For more information about Keras go to <https://keras.io/>\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "We'll use the same data as for clustering. However, for this exercise we need training and testing samples, so that we can test how well our model performs. Test data is useful to observe that our model is not only memorizing the samples, but it should be able to classify unseen data. Therefore, we don't provide the model with labels in the test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# We are already familiar with the load_data function, it returns train and test data in tuples.\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# y_train is an array of length 60000, each row containing a label\n",
    "print (\"y_train.shape = {}\".format(y_train.shape))\n",
    "# x_train is an array of shape 60000x28x28, each of the 60k entries is a 2-dimensional array of 28x28\n",
    "# containing each image as a matrix of gray-scaled pixels.\n",
    "print (\"x_train.shape = {}\".format(x_train.shape))\n",
    "print(\"Example of an element in y_i: {}\".format(y_train[0]))\n",
    "print(\"Example of an element in x_i: {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "As before, we flatten the 2D data into a string of single values.\n",
    "But we also have to pre-process the labels. The learning process does not expect class indices (1,2, ..., 9) but the very popular one-hot vectors. This kind of vectors, holds each class has its own dimension and only the label dimension has the value one:\n",
    "\n",
    "0 &rarr; [1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "1 &rarr; [0,1,0,0,0,0,0,0,0,0]\n",
    "\n",
    "...\n",
    "\n",
    "9 &rarr; [0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "The separate dimensions provide a more meaningful error value and are easy to generalize.\n",
    "\n",
    "And it is preferable to normalize the image values, from a range between 0 - 255 to a range between 0 to 1.\n",
    "\n",
    "***Hint:***\n",
    "- `keras.utils.np_utils` contains a function that transforms labels to one hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# 10 categories in our data\n",
    "num_class = 10\n",
    "\n",
    "# Normalizing color values\n",
    "x_train = x_train / float(255)\n",
    "x_test  = x_test / float(255)\n",
    "\n",
    "# one hot vectors of the shape 10 ()\n",
    "y_train_hot = np_utils.to_categorical(y_train, 10)\n",
    "y_test_hot  = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Flatten the data in 784 separate dimensions\n",
    "x_train = x_train.reshape(len(x_train),28*28)\n",
    "x_test = x_test.reshape(len(x_test),28*28)\n",
    "\n",
    "# Shapes \n",
    "print (\"y_train_hot.shape = {}\".format(y_train_hot.shape))\n",
    "print (\"x_train.shape = {}\".format(x_train.shape))\n",
    "\n",
    "# Examples \n",
    "print(\"Example of an element in y_i: {}\".format(y_train_hot[0]))\n",
    "print(\"Example of an element in x_i: {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model\n",
    "\n",
    "Now we have to define the structure of the neural net. The elements in the brackets build the model architecture and were choosen for the following reasons:\n",
    "\n",
    "- Our network has a sequential control flow and no recursions.(`keras.models.Sequential`)\n",
    "\n",
    "- Each layer consists of two parts, a connection to the previous layer (`keras.layers.Dense`) and an activation function (`keras.layers.Activation`)\n",
    "\n",
    "- The `Dense` layers include the number of nodes, which is automatically the input shape to the next layers. \n",
    "\n",
    "- Given the fact that the first layer doesn't have any previous layers, we have to implement the number of input nodes. This number has to fit our data structure. In our case there are 784 separate values, which we define through the parameter `input_shape`.\n",
    "\n",
    "- The last layer includes as much outputs as classes contained in the dataset.\n",
    "\n",
    "In each layer we should pass a quantified activation. This is done by the `relu` activation function. In our case the last layer should choose only one class. Therefore we use the `softmax` activation function.\n",
    "\n",
    "For further parameters (loss, optimizer, metrics, ...) we take default values or we have already declared useful values for you. \n",
    "\n",
    "For another step we import the TensorBoard library, so that we can visualize our results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=x_train[0].shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_class)) # 10 outputs\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Generates a graph event to visualize the control flow.\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "# Summarizes the settings and outputs the complexity of our model.\n",
    "# In other words, how many weights have influence to the output.\n",
    "# The more degrees of freedom, the more labeled data should be present.\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training itself is nothing special. We use the method `model.fit` and define the relevant data: \n",
    "- the data(x)\n",
    "- the labels as one-hot vectors(y)\n",
    "- the number of iterations(epochs)\n",
    "- the batch size(batch_size)\n",
    "\n",
    "If you enter the test data as `validation data`, we get the calculated model quality after each epoch on the basis of the test data.\n",
    "\n",
    "Epochs are iterations over all data points. The less data we have the more we have to iterate to improve the weights often enough. With more epochs the learning process receives the same data multiple times. There is a risk that the model memorizes the patterns and doesn't generalize any more. \n",
    "The batch size defines the number of instances, whose error is examined by the optimizer before the weights will be adapted. \n",
    "\n",
    "***Hint:*** The fit-method delivers the history, which allows us to visualize the training process. Furthermore this method includes a `callbacks` attribute, it is fed with the tensorboard object and enables us an access to the Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train_hot, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(x_test, y_test_hot),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Progress of accuracy \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training accuracy increases during the epochs, because the model adapts to the training data. \n",
    "At some point in the training this is not the case anymore for the test data and it could become even worse. \n",
    "So we have to consider not overtraining the model (*overfitting*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "Another visualisation method is called *Tensorboard*. To understand, debug, and optimize your model the Tensorbaord includes some visualization tools. You can inspect your computation graph, or plot quantitative metrics like the accuracy or the loss function.\n",
    "\n",
    "To open Tensorboard you have to proceed the following way:\n",
    "- Type in this command in your docker terminal to match port 6006 to your container: \n",
    "\n",
    "    `docker run -p 0.0.0.0:6006:6006 -it novatec/mlss bash`\n",
    "\n",
    "\n",
    "- Start Tensorboard in your log directory: \n",
    "\n",
    "    `tensorboard --logdir=exercises/logs/`\n",
    "  \n",
    "  \n",
    "- Open port 6006 in your browser (copy and paste): \n",
    "\n",
    "(***replace <your_docker_ip> by your own docker-ip***)\n",
    "\n",
    "    `http://<your_docker_ip>:6006`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 0.0.0.0:6006:6006 -it novatec/mlss bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The information during the training was already promising. As in previous exercises, let's take a look at the confusion matrix to estimate the numbers. In order to do so, we use our already trained model and let it make predictions for the test data.\n",
    "\n",
    "***Hint:***\n",
    "- `predict_classes` returns the labels directly, saving the conversion of one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "predictions = model.predict_classes(x_test);\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test ,predictions)\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "\n",
    "# Output\n",
    "print(\"ACC: {}\".format(accuracy))\n",
    "print(\"CM: {}\".format(cm))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks awesome! Let's save the model so that we can use it again at any time without any effort. New data can now be preprocessed in the same way and classified using `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('MyFashionClassifier.h5') # Save the current status in a HDF5 format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
