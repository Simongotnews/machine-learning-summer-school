{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "As we already know from our lectures, the absence of labels, or a \"gold standard\" from outside, constitutes unsupervised learning. This makes even more important to become your own feeling for the data.\n",
    "In this notebook we want to analyze Zalando's Fashion-MNIST data-set with unsupervised learning methods.\n",
    "Our approach:\n",
    "1. Collecting Data \n",
    "2. Visualisation\n",
    "3. Clustering\n",
    "4. Evaluation\n",
    "5. Outliers Detection\n",
    "\n",
    "## Collecting Data\n",
    "\n",
    "Since we use a standard data set, this step is very simple. Keras (and other toolkits) have functions to load these data sets easily. Given that this data-set with 70,000 images is quite extensive, we initially only work with the test data set (10,000 images). We'll then call the pictures \"instances\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# load_data returns a nested tuple so that we can already assing \n",
    "# separate variables at the moment of calling it.\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Since we're only using test, we can just get rid of long names\n",
    "x = x_test\n",
    "y = y_test\n",
    "\n",
    "# y is an array of length 10000 containing a label in each row\n",
    "print (\"y = {}, y.shape = {}\".format(y, y.shape))\n",
    "\n",
    "# x is also an array, in this case its shape is 10000x28x28. Each row contains \n",
    "# a 2-dimensional array (an image) of shape 28x28 with pixel values.\n",
    "\n",
    "print (\"x.shape = {}\".format(x.shape))\n",
    "\n",
    "# We can also use the function \"?\" for detailed information about *x* \n",
    "x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "Using `matplotlib.pyplot.imshow` we can display one of the pictures (here we only look at first one). The range colors is not actually intended for images, but we can change it to gray values. \n",
    "\n",
    "Most methods cannot deal with the 2-dimensional structure of an image, since each value of the data spans its own dimension. We need to reshape the 28 x 28 pixels into a list of 784 values (28*28).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "# Display picture\n",
    "picture_nr = 0\n",
    "plt.imshow(x_test[picture_nr])\n",
    "plt.show()\n",
    "\n",
    "# Plot the data into 784 separate dimensions\n",
    "x = x_test.reshape(len(x),28*28)\n",
    "print (\"reshaped: {} -> {}\".format(x_test.shape,x.shape))\n",
    "\n",
    "# Of course we are not able to display our image like this anymore. \n",
    "# We need to reverse this transformation. Here even the correct range of colors\n",
    "plt.imshow(x[picture_nr].reshape(28,28), cmap='binary', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters with K-Means\n",
    "\n",
    "\n",
    "The basic idea of K-Means clustering is simple and easy to rebuild. \n",
    "For each (initially randomly generated) cluster, a \"centroid\" is calculated as the mean value of all instances belonging to the cluster. Then all instances are reassigned to the clsuters by looking at the distace between each intance and its closest centroid. These two steps are iterated until there are no more changes.\n",
    "\n",
    "K stands for the number of clusters and must be defined beforehand. This quantity must be determined either experimentally (at what value do the results no longer improve?) or, as here, is specified. \n",
    "\n",
    "First step: Find out a good value for *k* by looking how many different labels there are in the dataset.\n",
    "\n",
    "***Hint:*** `numpy.bincount()` counts how often each element occurs in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes do we actually have?\n",
    "import numpy as np\n",
    "\n",
    "print( ... ) # -> Please fill in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST has 10 classes and each class appears exactly 1000 times in the test data. So we try to divide the data into 10 groups according to their brightness values:\n",
    "\n",
    "***Hint***\n",
    "- you can refer to this pseudocode\n",
    " ![kmeans](../data/kmeans-alg.png)\n",
    " \n",
    " \n",
    "- However, for this dataset it makes more sense to start the centroids with completly random data instead a sampling.\n",
    "- rely on numpy for distance calculation(euclidian norm), finding minima and the like.\n",
    "- after you have implemented it yourself: you can compare it to the results of `sklearn.cluster.KMeans` `KMeans.fit_predict()`. While the basic algorithm is simple, there are many optimizations possible (foremost: better initialization). Thus it  usually makes sense to stick to a framework implentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -> Please complete\n",
    "\n",
    "# some constants\n",
    "...\n",
    "\n",
    "# some variables\n",
    "...\n",
    "\n",
    "# init centroids\n",
    "centers = ...\n",
    "\n",
    "# main loop\n",
    "...\n",
    "\n",
    "print (y_kmeans)\n",
    "print (centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how realistic is our result\n",
    "\n",
    "Let's see how the distribution fits to the groud truth. If you have saved the membership array (the actual clusterung result) in the variable `y_kmeans`, the following code block creates a bar chart of the cluster sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), np.bincount(y_kmeans))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so well, because our evenly distributed dataset is clustered into differently sized clusters. Lets look further into the acutal model: the centroids. Given that they are living in the same \"data space\" as the instances, we can simply display them as images to get a feeling of what did the model \"understand\" of the data.\n",
    "\n",
    "Plot the centroids using the same plotting code as in the beginning of this exercise.\n",
    "\n",
    "***Hint***\n",
    "- Don't forget the `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Please insert your contribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this doesn't look so bad, except for some classes with the \"ghostly\" shirts and shoes as centroid. To investigate further, we luckily have the correct answers in varaible `y`. Unfortunately, we cannot just compare them, as the order of our groups is arbitrary. \n",
    "\n",
    "Now, we have to go through our groups and look which ground truth label is the most fitting one.\n",
    "\n",
    "***Hint***\n",
    "- `numpy.zeros_like(X)` creates an empty array with the same dimensionality as *X*\n",
    "- `scipy.stats.mode(X)` calculates the mode ( = most common value) of the array *X*\n",
    "- with `(X == y)` the positions of the array *X* can be found, which have the (scalar) value *y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Please insert your contribution\n",
    "\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the mapping you can see that some groups point to the same label. For instance, two different variants of *Ankle Boots* (index 9) were distinguished, while other classes went lost.\n",
    "\n",
    "Now we can also calculate how many mistakes we actually commited (see also the extra exercise file *Evaluation*).\n",
    "You can count the true possitives by comparing the truth (y) with the assignment (from the code block above) or have scikit_learn calculate the *accuracy*.\n",
    "Instead of just the number of correct answers, a confusion matrix enables a better insight into which class is causing problems.\n",
    "\n",
    "***Hint***\n",
    "- `sklearn.metrics` contains the evaluation functions\n",
    "- `matplotlib.pyplot.matshow` can be used to visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Please complete\n",
    "\n",
    "import ...\n",
    "\n",
    "# Option 1 from scratch:\n",
    "...\n",
    "\n",
    "# Option 2 sklearn:\n",
    "...\n",
    "\n",
    "# Output\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing some white spots on the main diagonal. These are the classes that cause the most problems. *Sweaters* and *dress* seem hard to tell apart from *T-shirts* and *Tops*.\n",
    "\n",
    "## Detecting outliers\n",
    "\n",
    "In order to examine outliers we now limit ourselves to a rather homogeneous class, so that the outliers become more obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1 # Trousers \n",
    "\n",
    "# Reduce to one class\n",
    "x_oc = x[y_test == index]\n",
    "print(x_oc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for outliers is always an attempt to define the \"normal\". A simple solution is to look at the neighbours. If I am like my neighbours, then I am \"normal\". The *Local Outlier Factor* calculates if I am isolated from my neighbours in relation to how close my neighbours are to each other.\n",
    "\n",
    "***Hint:***\n",
    "- `sklearn.LocalOutlierFactor` can be used like KMeans (`fit_predict)` and returns -1 for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Please insert your contribution\n",
    "...\n",
    "print(np.histogram(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some pictures of the outlier class and some of the normal class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Please insert your contribution\n",
    "\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "  <summary>Spoiler Warning! The result - Click here to show </summary>\n",
    "   These are bell bottoms, two trousers in a pictures and even body images among the exceptions. Up to now, we have calculated all this only on gray values. By putting more effort into an optimal representation of the data, we could improve the results even more.\n",
    "</details>\n",
    "\n",
    "Feel free to analyse other classes, other clustering algorithms (`DBScan`) or try to familiarize yourself with dimension reduction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
