{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Perceptron Classifier from Scratch\n",
    "\n",
    "After having seen several classifiers it's our turn to implement one of them by ourselves. A perceptron is a good choice to start implementing it from scratch because it is easy to understand. Defining algorithms from scratch allows us to gain a deeper understanding of what actually happens inside the black box.\n",
    "\n",
    "Let's start by thinking about a perceptron as a function that maps an input `X` to an output `y`. In most of the cases, `X` is a set of features $x_1$, $x_2$, ... $x_n$ put together. For instance, in an image, our features are pixel values and in text processing, some characteristics like uppercased words, or part-of-speech tags (the word is a verb, or a noun) can be included as features. In order to do this exercise simple to understand, we'll create our own data and at the end, you will be able to play with a bigger dataset.\n",
    "\n",
    "Let's display some random features in a table and create the data for them:\n",
    "\n",
    "Features and labels:\n",
    "\n",
    " ![table](../data/table.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your x-matrix and your y-vector\n",
    "\n",
    "x = np.array([[2., 5.],\n",
    "              [0., 2.],\n",
    "              [2., 3.],\n",
    "              [5., 1.]])\n",
    "# Labels \n",
    "y = np.zeros(4)\n",
    "y[0] = 1\n",
    "y[1] = 1\n",
    "   \n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our function:\n",
    "\n",
    "![funct](../data/funct.png)\n",
    "\n",
    "We need to find a set of weights, that multiplied with the feature vectors return a value that would help us to classify the input data.\n",
    "\n",
    "**How?**\n",
    "- Define threshold, learning rate and number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "threshold = 0.0     \n",
    "alpha = 0.1   \n",
    "epochs = 50   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step: we need to initialize our weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the perceptron\n",
    "It's time to build your perceptron function, remember:\n",
    "\n",
    "- Generate a weight vector $w$ and a prediction vector $\\widehat{y}$ \n",
    "- Initialize $w$\n",
    "\n",
    "And let's start our iterations!\n",
    "\n",
    "- Define your function $f$\n",
    "- Assign a value to $\\widehat{y}$\n",
    "- Compare $\\widehat{y}$ and $y$\n",
    "- Update weights $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(x, y, threshold, alpha, epochs): \n",
    "    # Define your weight vector and prediction vector\n",
    "    w = np.zeros(len(x[0]))\n",
    "    \n",
    "    yhat_vec = np.ones(len(y))  \n",
    "    \n",
    "    \n",
    "    for iteration in range(epochs):   \n",
    "        converge = True\n",
    "        for i in range(0, len(x)):                 \n",
    "\n",
    "            # Elementwise multiplication step\n",
    "            f = np.dot(x[i], w)                      \n",
    "\n",
    "            # prediction\n",
    "            if f > threshold:                               \n",
    "                yhat = 1.                               \n",
    "            else:                                   \n",
    "                yhat = 0.\n",
    "            yhat_vec[i] = yhat                              \n",
    "\n",
    "            # weights update\n",
    "            if yhat != y[i]:\n",
    "                w = w + alpha*(y[i]-yhat)*x[i]\n",
    "                converge = False\n",
    "        if converge == True:\n",
    "            break\n",
    "\n",
    "\n",
    "            iteration += 1             \n",
    "    return w, yhat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, y_train = train_perceptron(x, y, threshold, alpha, epochs)\n",
    "print(\"Here are my weights:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to plot our classification. We can use our weights to plot our decision boundary.\n",
    "\n",
    "Let's consider the function $f(x)= w*x$. Please notice, that this example is deliberately designed to work even without considering a bias, we'll use that in the later example. Now, let's take a look of what happens inside this elementwise multiplication: $f(x)= w_0*x_0+w_1*x1$.\n",
    "In order to figure out our decision boundary, we need to find at least two point that match exactly our line. \n",
    "\n",
    "- We also need the minimum and maximum coordinates in $x_0$ of all our vector points. \n",
    "\n",
    "- Now, let's set our previous equation to zero and solve regarding $x_1$. This will deliver the coordinates  $x_1$ of the minima and maxima in our hyperplane. \n",
    "\n",
    "- A Plot of a line between those two vectors would display our decision boundary\n",
    "\n",
    "\n",
    "### $x_1=\\frac{-w_0*x_0}{w_1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find min and max for x_0\n",
    "p1_x = np.min(x[:,0])\n",
    "p2_x = np.max(x[:,0])\n",
    "\n",
    "# Find coordinates in x_1\n",
    "p1_y = -w[0]*p1_x/w[1]\n",
    "p2_y = -w[0]*p2_x/w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "classification = np.linspace(1,len(w),len(w)) \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(x[:,0], x[:,1], edgecolors=(1, 0, 0))\n",
    "ax1.plot([p1_x, p2_x], [p1_y,p2_y], lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "We need to test if those weights actually work, let's write a test function for the perceptron. \n",
    "\n",
    "**Remember!**\n",
    "\n",
    "Here we do not iterate, and do not update weights, we just need a prediction. This means only the $\\widehat{y}$-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perceptron(x, threshold, alpha, w):\n",
    "    yhat_vec = [] \n",
    "    \n",
    "    for i in range(0, len(x)):              \n",
    "        \n",
    "        f = np.dot(x[i], w) \n",
    "        \n",
    "        if f > threshold:                               \n",
    "            yhat = 1.                               \n",
    "        else:                                   \n",
    "            yhat = 0.\n",
    "        \n",
    "        yhat_vec.append(yhat)\n",
    "        \n",
    "    return yhat_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! We just need to feed some test our perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1., 4.],\n",
    "              [1., 2.],\n",
    "              [4., 2.],\n",
    "              [3., 1.]])\n",
    "# Labels \n",
    "test_y = np.zeros(4)\n",
    "test_y[0] = 1\n",
    "test_y[1] = 1\n",
    "\n",
    "pred_y = test_perceptron(x, threshold, alpha, w)\n",
    "print(pred_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to compare the prediction delivered with our gold labels. Let's compute the accuracy of the model and see how does the perceptron work on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(np.equal(test_y, pred_y))/len(test_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We made it! It looks like a peceptron can classify very good our data. What about creating some more data and testing it again? For that, we can use the function `multivariate_normal` provided in `numpy.random`, this function takes as input a vector as *mean* and a matrix as *covariance matrix* and delivers a lot of random samples around the mean. The covariance matrix determines the magnitude and the main axes of the samples' distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "\n",
    "class_1 = np.random.multivariate_normal([6,-6], [[1.,.5],[.5,1.]], samples)\n",
    "class_2 = np.random.multivariate_normal([2, 2], [[1.,.5],[.5,1.]], samples)\n",
    "\n",
    "x_data = np.vstack((class_1, class_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these samples look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.scatter([class_1[:,0]], [class_1[:,1]], c=\"b\")\n",
    "ax2.scatter([class_2[:,0]], [class_2[:,1]], c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Let's create the labels include a bias and merge all data together. The bias is just a value which is constant for all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "labels_1 = np.ones((samples,1))\n",
    "labels_2 = np.zeros((samples,1))\n",
    "\n",
    "# Create bias and merge everything together\n",
    "bias = np.ones((samples*2, 1)).astype(np.float32)\n",
    "labels = np.vstack((labels_1, labels_2))\n",
    "features = np.concatenate((class_1, class_2), axis=0)\n",
    "biased = np.hstack((bias, features))\n",
    "dataset = np.hstack((biased, labels))\n",
    "\n",
    "# Shuffle data and apply a 80/20 split\n",
    "np.random.shuffle(dataset)\n",
    "split = int(len(dataset[:,0]) * 0.8)\n",
    "train = dataset[:split,:]\n",
    "test = dataset[split:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your parameters and train again to obtain the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "epochs = 1000\n",
    "alpha = 0.01\n",
    "threshold = 0\n",
    "train_data = train[:,0:3]\n",
    "train_labels = train[:,3]\n",
    "\n",
    "test_data = test[:,0:3]\n",
    "test_labels = test[:,3:]\n",
    "\n",
    "w, train_y = train_perceptron(train_data, train_labels, threshold, alpha, epochs)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just plot again our decision boundary, remember, that this time we have a bias, so our formula looks a bit different:   \n",
    "## $x_1=\\frac{-w_0-w_1*x_0}{w_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1_x = np.min(train_data[:,1])\n",
    "p2_x = np.max(train_data[:,1])\n",
    "\n",
    "p1_y = (threshold-w[0]-w[1]*p1_x)/w[2]\n",
    "p2_y = (threshold-w[0]-w[1]*p2_x)/w[2]\n",
    "\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.scatter(train_data[:,1], train_data[:,2], c=\"b\")\n",
    "ax3.plot([p1_x, p2_x], [p1_y, p2_y])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about accuracy? If this graph is correct, we have an accuracy of 100% Yay! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = test_perceptron(test_data, threshold, alpha, w)\n",
    "pred_y = np.reshape(pred_y,(len(pred_y),1))\n",
    "accuracy = np.sum(np.equal(test_labels, pred_y))/len(pred_y)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
