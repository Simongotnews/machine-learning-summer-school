{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "As we already know from our lectures, the absence of labels, or a \"gold standard\" from outside, constitutes unsupervised learning. This makes even more important to become your own feeling for the data.\n",
    "In this notebook we want to analyze Zalando's Fashion-MNIST data-set with unsupervised learning methods.\n",
    "Our approach:\n",
    "1. Collecting Data \n",
    "2. Visualisation\n",
    "3. Clustering\n",
    "4. Evaluation\n",
    "5. Outliers Detection\n",
    "\n",
    "## Collecting Data\n",
    "\n",
    "Since we use a standard data set, this step is very simple. Keras (and other toolkits) have functions to load these data sets easily. Given that this data-set with 70,000 images is quite extensive, we initially only work with the test data set (10,000 images). We'll then call the pictures \"instances\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# load_data returns a nested tuple so that we can already assing \n",
    "# separate variables at the moment of calling it.\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Since we're only using test, we can just get rid of long names\n",
    "x = x_test\n",
    "y = y_test\n",
    "\n",
    "# y is an array of length 10000 containing a label in each row\n",
    "print (\"y = {}, y.shape = {}\".format(y, y.shape))\n",
    "\n",
    "# x is also an array, in this case its shape is 10000x28x28. Each row contains \n",
    "# a 2-dimensional array (an image) of shape 28x28 with pixel values.\n",
    "\n",
    "print (\"x.shape = {}\".format(x.shape))\n",
    "\n",
    "# We can also use the function \"?\" for detailed information about *x* \n",
    "x?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "Using `matplotlib.pyplot.imshow` we can display one of the pictures (here we only look at first one). The range colors is not actually intended for images, but we can change it to gray values. \n",
    "\n",
    "Most methods cannot deal with the 2-dimensional structure of an image, since each value of the data spans its own dimension. We need to reshape the 28 x 28 pixels into a list of 784 values (28*28).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "# Display picture\n",
    "picture_nr = 0\n",
    "plt.imshow(x_test[picture_nr])\n",
    "plt.show()\n",
    "\n",
    "# Plot the data into 784 separate dimensions\n",
    "x = x_test.reshape(len(x),28*28)\n",
    "print (\"reshaped: {} -> {}\".format(x_test.shape,x.shape))\n",
    "\n",
    "# Of course we are not able to display our image like this anymore. \n",
    "# We need to reverse this transformation. Here even the correct range of colors\n",
    "plt.imshow(x[picture_nr].reshape(28,28), cmap='binary', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters with K-Means\n",
    "\n",
    "\n",
    "The basic idea of K-Means clustering is simple and easy to rebuild. \n",
    "For each (initially randomly generated) cluster, a \"centroid\" is calculated as the mean value of all instances belonging to the cluster. Then all instances are reassigned to the clsuters by looking at the distace between each intance and its closest centroid. These two steps are iterated until there are no more changes.\n",
    "\n",
    "K stands for the number of clusters and must be defined beforehand. This quantity must be determined either experimentally (at what value do the results no longer improve?) or, as here, is specified. \n",
    "\n",
    "First step: Find out a good value for *k* by looking how many different labels there are in the dataset.\n",
    "\n",
    "***Hint:*** `numpy.bincount()` counts how often each element occurs in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes do we actually have?\n",
    "import numpy as np\n",
    "\n",
    "print(np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion-MNIST has 10 classes and each class appears exactly 1000 times in the test data. So we try to divide the data into 10 groups according to their brightness values:\n",
    "\n",
    "***Hint***\n",
    "- you can refer to the pseudocode in the slides (see, folder \"slides\")\n",
    "- However, for this dataset it makes more sense to start the centroids with completly random data instead a sampling.\n",
    "- rely on numpy for distance calculation(euclidian norm), finding minima and the like.\n",
    "- after you have implemented it yourself: you can compare it to the results of `sklearn.cluster.KMeans` `KMeans.fit_predict()`. While the basic algorithm is simple, there are many optimizations possible (foremost: better initialization). Thus it  usually makes sense to stick to a framework implentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "maxIter = 100\n",
    "k = 10\n",
    "\n",
    "# some variables\n",
    "iteration = 0\n",
    "converged = False\n",
    "m = np.zeros(len(x),dtype='int64')\n",
    "\n",
    "# init centroids\n",
    "centers = np.random.random_integers(0,255,10*28*28) # create 10*28*28 random values between 0 and 255\n",
    "centers = centers.reshape(10,28*28) # shape them into 10 rows.\n",
    "\n",
    "# main loop\n",
    "while (not converged and iteration < maxIter):\n",
    "    iteration += 1\n",
    "    old_m = m.copy()\n",
    "    # update members\n",
    "    for i in range(len(x)):\n",
    "        # calculate distances\n",
    "        distances = np.linalg.norm(centers - x[i],axis=1)\n",
    "        m[i] = np.argmin(distances)\n",
    "    # update centroids\n",
    "    for k_i in range(k):\n",
    "        members = x[m==k_i]\n",
    "        centers[k_i] = np.mean(members,axis=0)\n",
    "    converged = np.min(np.equal(old_m,m))\n",
    "    print(\"Iteration {} finished, converged: {}\".format(iteration,converged))\n",
    "\n",
    "print(\"Done after {} iterations\".format(iteration))\n",
    "y_kmeans = m\n",
    "\n",
    "# same with scikit_learn\n",
    "#from sklearn.cluster import KMeans\n",
    "#kmeans = KMeans(n_clusters=10)\n",
    "#y_kmeans = kmeans.fit_predict(x)\n",
    "\n",
    "print (y_kmeans)\n",
    "print (centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how realistic is our result\n",
    "\n",
    "Let's see how the distribution fits to the groud truth. If you have saved the membership array (the actual clusterung result) in the variable `y_kmeans`, the following code block creates a bar chart of the cluster sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(k), np.bincount(y_kmeans))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so well, because our evenly distributed dataset is clustered into differently sized clusters. Lets look further into the acutal model: the centroids. Given that they are living in the same \"data space\" as the instances, we can simply display them as images to get a feeling of what did the model \"understand\" of the data.\n",
    "\n",
    "Plot the centroids using the same plotting code as in the beginning of this exercise.\n",
    "\n",
    "***Hint***\n",
    "- Don't forget the `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "#centers = kmeans.cluster_centers_.reshape(10, 28, 28)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center.reshape(28, 28), interpolation='nearest', cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this doesn't look so bad, except for some classes with the \"ghostly\" shirts and shoes as centroid. To investigate further, we luckily have the correct answers in varaible `y`. Unfortunately, we cannot just compare them, as the order of our groups is arbitrary. \n",
    "\n",
    "Now, we have to go through our groups and look which ground truth label is the most fitting one.\n",
    "\n",
    "***Hint***\n",
    "- `numpy.zeros_like(X)` creates an empty array with the same dimensionality as *X*\n",
    "- `scipy.stats.mode(X)` calculates the mode ( = most common value) of the array *X*\n",
    "- with `(X == y)` the positions of the array *X* can be found, which have the (scalar) value *y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "label_prediction = np.zeros_like(y_kmeans)\n",
    "class_mapping = {};\n",
    "for i in range(10):\n",
    "    # Select all the instaces that were assigned to group i and save this selection\n",
    "    mask = (y_kmeans == i) \n",
    "    # Looks for the correct labels of this instance (y[mask]) and \n",
    "    # takes the most common value (mode()[0][0])\n",
    "    matching_label = mode(y[mask])[0][0];\n",
    "    # assings this value only to the selected positions in label_prediction (labels[mask])\n",
    "    label_prediction[mask] = matching_label\n",
    "    # Now we can store this mapping\n",
    "    class_mapping[i] = matching_label\n",
    "    \n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the mapping you can see that some groups point to the same label. For instance, two different variants of *Ankle Boots* (index 9) were distinguished, while other classes went lost.\n",
    "\n",
    "Now we can also calculate how many mistakes we actually commited (see also the extra exercise file *Evaluation*).\n",
    "You can count the true possitives by comparing the truth (y) with the assignment (from the code block above) or have scikit_learn calculate the *accuracy*.\n",
    "Instead of just the number of correct answers, a confusion matrix enables a better insight into which class is causing problems.\n",
    "\n",
    "***Hint***\n",
    "- `sklearn.metrics` contains the evaluation functions\n",
    "- `matplotlib.pyplot.matshow` can be used to visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Option 1 from scratch:\n",
    "# init \n",
    "cm   = np.zeros((10,10), dtype=int)\n",
    "hits = 0\n",
    "# Counting all predictions\n",
    "for i in range(len(y)):\n",
    "    index_truth = y[i]\n",
    "    index_preds = label_prediction[i]\n",
    "    cm[index_truth][index_preds] += 1\n",
    "    if (index_truth == index_preds):\n",
    "        hits += 1\n",
    "accuracy = hits / len(y)\n",
    "\n",
    "# Option 2 sklearn\n",
    "cm = metrics.confusion_matrix(y, label_prediction)\n",
    "accuracy = metrics.accuracy_score(y, label_prediction)\n",
    "\n",
    "# Output\n",
    "print(\"ACC: {}\".format(accuracy))\n",
    "print(\"CM: {}\".format(cm))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing some white spots on the main diagonal. These are the classes that cause the most problems. *Sweaters* and *dress* seem hard to tell apart from *T-shirts* and *Tops*.\n",
    "\n",
    "## Detecting outliers\n",
    "\n",
    "In order to examine outliers we now limit ourselves to a rather homogeneous class, so that the outliers become more obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1 # Trousers \n",
    "\n",
    "# Reduce to one class\n",
    "x_oc = x[y_test == index]\n",
    "print(x_oc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for outliers is always an attempt to define the \"normal\". A simple solution is to look at the neighbours. If I am like my neighbours, then I am \"normal\". The *Local Outlier Factor* calculates if I am isolated from my neighbours in relation to how close my neighbours are to each other.\n",
    "\n",
    "***Hint:***\n",
    "- `sklearn.LocalOutlierFactor` can be used like KMeans (`fit_predict)` and returns -1 for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Inside or outside?\n",
    "lof = LocalOutlierFactor(n_neighbors=30)\n",
    "y_outfact = lof.fit_predict(x_oc)\n",
    "\n",
    "# How many outliers and \n",
    "# how many normal ones do we have?\n",
    "print(np.histogram(y_outfact, bins=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some pictures of the outlier class and some of the normal class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst = lof.negative_outlier_factor_.argsort()[:24]\n",
    "best  = lof.negative_outlier_factor_.argsort()[len(x_oc)-24:]\n",
    "\n",
    "print(len(worst))\n",
    "print(len(best))\n",
    "\n",
    "print(\"Outliers:\")\n",
    "fig, ax = plt.subplots(3, 8, figsize=(24,9))\n",
    "for axi, i_img in zip(ax.flat, worst):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(x_oc[i_img].reshape(28,28), interpolation='nearest', cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normals:\")\n",
    "fig, ax = plt.subplots(3, 8, figsize=(24,9))\n",
    "for axi, i_img in zip(ax.flat, best):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(x_oc[i_img].reshape(28,28), interpolation='nearest', cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "  <summary>Spoiler Warning! The result - Click here to show </summary>\n",
    "   These are bell bottoms, two trousers in a pictures and even body images among the exceptions. Up to now, we have calculated all this only on gray values. By putting more effort into an optimal representation of the data, we could improve the results even more.\n",
    "</details>\n",
    "\n",
    "Feel free to analyse other classes, other clustering algorithms (`DBScan`) or try to familiarize yourself with dimension reduction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
