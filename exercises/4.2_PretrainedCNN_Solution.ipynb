{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with pretrained CNNs\n",
    "\n",
    "In this notebook we want to use a CNN with our prepared image dataset. So far we used the nice and clean Fashion MNIST dataset to create a model. \n",
    "\n",
    "This time it's your turn to preprocess the raw, collected data!\n",
    "\n",
    "\n",
    "## Obtaining and Preprocessing the Data\n",
    "\n",
    "We already provide two image collections on the server for you.\n",
    "\n",
    "- In the folder [*/data/image_cats_dogs*](/tree/data/image_cats_dogs) are 100 cat and 100 dog images. They come from the Kaggle-Challenge [*Dogs vs. Cats*](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "- In the folder [*/data/image_lion_puma*](/tree/data/image_lion_puma) are some images of lions and pumas, intentionally badly chosen to show the limits of learning methods.  \n",
    "- If you trust yourself to create your own dataset, you can do it by the same procedure. For example you could develop a George Clooney vs. Brad Pitt classifier. Therefore you only have to upload some images of them on this Jupyter server.\n",
    "\n",
    "*** If you haven't already done it, you should execute the [exercise 1.2](/notebooks/exercises/1.2_ImageProcessing_Exercise.ipynb) for image processing the cat vs. dog dataset (or modify it to use the other dataset). After that we can start with this notebook and compare a simple CNN with a pretrained CNN. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our current path\n",
    "print(\"current path: {}\".format(os.getcwd()))\n",
    "\n",
    "# let's save the home path: this value is for the container environment. \n",
    "# You can update the path according to the outputof the command above\n",
    "home = \"/home/jovyan/\"\n",
    "\n",
    "# a mapping for human readable labels\n",
    "labels = { 0 : \"cat\", 1 : \"dog\"}\n",
    "\n",
    "# make sure you have the data ready: this should show some images\n",
    "folders = {home + \"temp/image_cats_dogs/train/dog/\"}\n",
    "for folder in folders:\n",
    "    for i, file in zip(range(3),os.listdir(folder)):\n",
    "        display(Image(filename=(folder + file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model \n",
    "\n",
    "Now we can start with the actual neural network itself. Therefore we need to import the deep learning libraries and create some constants for the dataset: (code is provided, just run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "img_size = 250\n",
    "\n",
    "num_train = 160\n",
    "num_test = 40\n",
    "\n",
    "train_data_dir = '/home/jovyan/temp/image_cats_dogs/train'\n",
    "test_data_dir  = '/home/jovyan/temp/image_cats_dogs/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We define a sequential network, which passes the data from input to the direction of output. \n",
    "\n",
    "The components of the Convolutional Network will be the same, as you've already seen in the exercises before.\n",
    "\n",
    "***Hints:***\n",
    "- the required elements are `Conv2D`, `Activation`, `MaxPooling`, `Flatten` and `Dense`\n",
    "- we have to compile the network definition with `Sequential.compile` and define the loss function as well as an optimizer \n",
    "- `Sequential.summary()` summarizes the layer and the number of degrees of freedom( = learning parameter)\n",
    "- start with two Convolutional Layer and reduce the dimension with Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the backend the order of dimension parameter is different\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_size, img_size)\n",
    "else:\n",
    "    input_shape = (img_size, img_size, 3)\n",
    "\n",
    "model = Sequential()\n",
    "Conv2D?\n",
    "model.add(Conv2D(32, (7, 7), input_shape=input_shape))\n",
    "#The model architecture could look as followed: \n",
    "#Conv2D, Activation, MaxPooling2D |Conv2D, Activation, MaxPooling2D | Flatten, Dense, Activation, Dense, Activation\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(7, 7)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "# in this case we have a binary classifier -> sigmoid with one output node\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training process\n",
    "\n",
    "Now we have to combine the data with the network. This is the same process like in the Fashion MNIST example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12 # we want to process each image twelve times, otherwise our augmentation would not make much sense. See below.\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be directly fed with data (`fit`) or can get the data from a generator (`fit_generator`). As we want to increase our data on-the-fly with augmentation, we define this generator and derive another generator form this previous generator, which reads the data in the training folder.\n",
    "\n",
    "This results in a pipeline in which we can ask for new images infinite times. Each time we ask for an image, the generator takes the next file in the training data folder, applies a random augmentation and supplies it as an input to the training loop.\n",
    "\n",
    "***Hints:***\n",
    "- The interger color values (0-255) should be normalized to the range from 0 to 1 with the `rescale` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=5,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can start the training process. Therefore we have to implement the `fit_generator` method with the training data generator and the desired number of epochs. After that we wait until the training is finished. Waiting is an important part of deep learning. Always have a good paper ready to read ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs)\n",
    "\n",
    "# Because we don't want to wait again, we store the resulting model in HDF5 format\n",
    "model.save_weights('MyCatDogConv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Progress of accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "Now we want to test the performance of our model for the test data. There is a method analogous to `fit_generator`, called `evaluate_generator`. \n",
    "\n",
    "This provides us with the usual evaluation metrics. If we want the actual prediction results, too, e.g.  to compute the confusion matrix, we can use the method `model.predict_generator`.\n",
    "\n",
    "***Hint***: Don't forget to `rescale` to the same value range (0-1) and set `shuffle` to `false` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=num_test,\n",
    "    shuffle=False,\n",
    "    class_mode='binary')\n",
    "\n",
    "scores = model.evaluate_generator(test_generator)\n",
    "predictions = model.predict_generator(test_generator)\n",
    "\n",
    "print(\"loss: {}, Acc: {} \".format(scores[0], scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are not binary value (because we used a sigmoid activation), but a value between 0 and 1. If we want assign a class, we employ a threshold (usually 0.5, but depending on the application one class can be more important.)\n",
    "\n",
    "Because you already done confusion matrices earlier, here is the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# we know that the test data starts with all cats and then all dogs\n",
    "truth = np.concatenate([np.zeros(20),np.ones(20)])\n",
    "\n",
    "# threshold = 0.5 --> round\n",
    "preds = np.around(predictions)\n",
    "\n",
    "# the common thing:\n",
    "print(\"Accuracy: {}\".format(skm.accuracy_score(truth,preds)))\n",
    "cm = skm.confusion_matrix(truth,preds)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+list(labels.values()))\n",
    "ax.set_yticklabels(['']+list(labels.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "With the limited datasets (cats vs. dogs and lions vs. pumas) the values probably wouldn't get any better. The task of training the computer to \"see\" from scratch is too difficult with so few examples. An alternative is taking a pretrained network, which is already trained on many image features and it can recognize them. We only take the last part of this pretrained CNN (Dense layers after convolutional layer, also called *Bottleneck*) and retrain them with our data according to our needs.\n",
    "\n",
    "The idea is, that the model has already learned how to see and understand image parts from working with many, many images. We only want to learn which image parts and features make up a dog or a cat.\n",
    "\n",
    "Therefore we download the current and already trained CNN (in our case: VGG16) without the upper layer and calculate the CNN output for our data without having trained it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "import numpy as np\n",
    "\n",
    "# no augmentation\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Pretrained CNN without the upper layers \n",
    "vgg16 = applications.VGG16(include_top=False, weights='imagenet', input_shape=(250,250,3))\n",
    "\n",
    "# Process the training data and save the ouput in a file \n",
    "generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_train = vgg16.predict_generator(\n",
    "        generator, num_train // batch_size)\n",
    "np.save(open(\"bottleneck_features_train.npy\", \"wb\"), bottleneck_features_train)\n",
    "\n",
    "# The same for test data\n",
    "generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "bottleneck_features_test = vgg16.predict_generator(\n",
    "        generator, num_test // batch_size)\n",
    "np.save(open(\"bottleneck_features_test.npy\", \"wb\"),\n",
    "            bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we model a very simple neural net and call it 'topModel'. This model shall learn what features make up a cat or dog.\n",
    "\n",
    "This time we directly integrate the evaluation into the training process by defining the test data, too (`validation_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CNN output again\n",
    "train_data = np.load(open(\"bottleneck_features_train.npy\", \"rb\"))\n",
    "train_labels = np.array([0] * (num_train // 2) + [1] * (num_train // 2))\n",
    "test_data = np.load(open(\"bottleneck_features_test.npy\", \"rb\"))\n",
    "test_labels = np.array([0] * (num_test // 2) + [1] * (num_test // 2))\n",
    "\n",
    "# define a small simple CNN\n",
    "topModel = Sequential()\n",
    "topModel.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "topModel.add(Dense(256, activation='relu'))\n",
    "topModel.add(Dropout(0.5))\n",
    "topModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "topModel.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# and train it\n",
    "topModel.fit(train_data, train_labels,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_data, test_labels))\n",
    "\n",
    "topModel.save_weights(\"top_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have great results, without augmentation and very few training examples!\n",
    "\n",
    "(to be fair, cats and dogs were already part of the pretraining, so this was an easy tasks. Finding guns and drugs would maybe be more difficult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
